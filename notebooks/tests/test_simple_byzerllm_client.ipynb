{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleByzerLLM 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from byzerllm.utils.client.simple_byzerllm_client import SimpleByzerLLM\n",
    "\n",
    "# 初始化客户端\n",
    "llm = SimpleByzerLLM(default_model_name=\"deepseek_chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'deepseek_chat', 'status': 'deployed'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 部署模型\n",
    "deploy_result = llm.deploy(\n",
    "    model_path=\"\",\n",
    "    pretrained_model_type=\"saas/openai\",\n",
    "    udf_name=\"deepseek_chat\",\n",
    "    infer_params={\n",
    "        \"saas.base_url\": \"https://api.deepseek.com/v1\",\n",
    "        \"saas.api_key\": os.getenv(\"MODEL_DEEPSEEK_TOKEN\"),\n",
    "        \"saas.model\": \"deepseek-chat\"\n",
    "    }\n",
    ")\n",
    "deploy_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'deepseek-chat',\n",
       " 'backend': 'openai',\n",
       " 'max_model_len': 4097,\n",
       " 'support_stream': True,\n",
       " 'deploy_info': {'model_path': '',\n",
       "  'pretrained_model_type': 'saas/openai',\n",
       "  'infer_params': {'saas.base_url': 'https://api.deepseek.com/v1',\n",
       "   'saas.api_key': 'sk-2abc4f81cd9b4b64990713d8cf1082fd',\n",
       "   'saas.model': 'deepseek-chat'},\n",
       "  'sync_client': <openai.OpenAI at 0x13d787b50>,\n",
       "  'async_client': <openai.AsyncOpenAI at 0x13d7bccd0>,\n",
       "  'model': 'deepseek-chat',\n",
       "  'is_reasoning': False},\n",
       " 'model_deploy_type': 'saas',\n",
       " 'message_format': True,\n",
       " 'support_chat_template': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取模型元数据\n",
    "meta = llm.get_meta(model=\"deepseek_chat\")\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！我是一个由OpenAI开发的人工智能助手，旨在帮助用户解答问题、提供信息、执行任务和进行对话。我可以处理多种语言，并且能够理解和生成自然语言文本。我的知识库涵盖了广泛的主题，包括科学、技术、历史、文化、日常生活等，并且我会不断更新以保持信息的准确性。\\n\\n如果你有任何问题或需要帮助，无论是学习、工作还是生活中的问题，我都会尽力提供有用的答案和建议。请随时向我提问！'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试普通聊天\n",
    "response = llm.chat_oai(\n",
    "    conversations=[{\"role\": \"user\", \"content\": \"你好，介绍一下你自己\"}],\n",
    "    model=\"deepseek_chat\"\n",
    ")\n",
    "response[0].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "快速排序（Quick Sort）是一种高效的排序算法，采用分治法（Divide and Conquer）策略。以下是一个用Python实现的快速排序算法：\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    # 如果数组长度小于等于1，直接返回\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    \n",
      "    # 选择一个基准元素（通常选择第一个元素）\n",
      "    pivot = arr[0]\n",
      "    \n",
      "    # 分别创建三个列表：小于基准的、等于基准的、大于基准的\n",
      "    less = [x for x in arr[1:] if x < pivot]\n",
      "    equal = [x for x in arr if x == pivot]\n",
      "    greater = [x for x in arr[1:] if x > pivot]\n",
      "    \n",
      "    # 递归地对小于和大于基准的部分进行排序，并将结果合并\n",
      "    return quick_sort(less) + equal + quick_sort(greater)\n",
      "\n",
      "# 示例用法\n",
      "arr = [3, 6, 8, 10, 1, 2, 1]\n",
      "sorted_arr = quick_sort(arr)\n",
      "print(\"排序后的数组:\", sorted_arr)\n",
      "```\n",
      "\n",
      "### 代码说明：\n",
      "1. **基准选择**：我们选择数组的第一个元素作为基准（`pivot`）。\n",
      "2. **分区**：将数组分为三个部分：\n",
      "   - `less`：所有小于基准的元素。\n",
      "   - `equal`：所有等于基准的元素。\n",
      "   - `greater`：所有大于基准的元素。\n",
      "3. **递归排序**：对`less`和`greater`部分递归调用`quick_sort`函数。\n",
      "4. **合并结果**：将排序后的`less`、`equal`和`greater`部分合并成一个有序数组。\n",
      "\n",
      "### 示例输出：\n",
      "```python\n",
      "排序后的数组: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "### 时间复杂度：\n",
      "- **平均情况**：O(n log n)\n",
      "- **最坏情况**：O(n^2)（当每次选择的基准都是最大或最小元素时）\n",
      "\n",
      "### 空间复杂度：\n",
      "- O(n)（由于递归调用栈和额外的列表空间）\n",
      "\n",
      "这个实现是快速排序的一个简单版本，适合理解算法原理。在实际应用中，可能会使用更复杂的优化策略，如三数取中法选择基准、尾递归优化等。"
     ]
    }
   ],
   "source": [
    "# 测试流式聊天\n",
    "for chunk in llm.stream_chat_oai(\n",
    "    conversations=[{\"role\": \"user\", \"content\": \"写一个Python的快速排序算法\"}],\n",
    "    delta_mode=True\n",
    "):\n",
    "    print(chunk[0], end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 《最后的记忆》\n",
      "\n",
      "我睁开眼睛，映入眼帘的是一片纯白的天花板。消毒水的气味刺激着我的鼻腔，我意识到自己正躺在医院的病床上。\n",
      "\n",
      "\"你醒了。\"一个温和的男声传来。我偏过头，看见一位穿着白大褂的医生正站在床边，他的胸前别着一个金属铭牌，上面写着\"记忆修复中心\"。\n",
      "\n",
      "\"我...这是在哪里？\"我试图坐起来，却发现浑身无力。\n",
      "\n",
      "\"别着急。\"医生按住了我的肩膀，\"你刚刚经历了一场严重的车祸，我们不得不对你的记忆进行修复和重组。\"\n",
      "\n",
      "我皱起眉头，努力回想，却只能捕捉到一些零碎的片段：刺眼的车灯、刺耳的刹车声、破碎的挡风玻璃......\n",
      "\n",
      "\"你的记忆芯片在事故中受损严重，\"医生继续说道，\"我们不得不植入新的芯片来替代。现在，我需要你配合做一些测试，确保新芯片运转正常。\"\n",
      "\n",
      "我点点头，任由医生将各种仪器连接到我的太阳穴。突然，一阵剧烈的头痛袭来，我的视野开始扭曲，耳边响起尖锐的蜂鸣声。\n",
      "\n",
      "\"不...不对劲......\"我痛苦地捂住头，\"这不是我的记忆......\"\n",
      "\n",
      "在意识模糊的边缘，我看到了一个陌生的实验室，无数个\"我\"躺在培养舱中，等待着被植入虚假的记忆......\n",
      "\n",
      "\"实验体出现异常反应！立即终止测试！\"医生的声音变得尖锐而慌乱。\n",
      "\n",
      "但已经太迟了。随着一声巨响，整个病房陷入黑暗。我知道，真相即将揭晓。\n"
     ]
    }
   ],
   "source": [
    "# 测试prompt装饰器\n",
    "@llm.prompt(model=\"deepseek_chat\")\n",
    "def generate_story(theme: str, length: int) -> str:\n",
    "    \"\"\"\n",
    "    根据主题生成一个故事\n",
    "    主题：{{theme}}\n",
    "    长度：{{length}}\n",
    "    \"\"\"\n",
    "\n",
    "story = generate_story(theme=\"科幻\", length=200)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "llm should be a lambda function or ByzerLLM instance or a string of model name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;129m@byzerllm\u001b[39m\u001b[38;5;241m.\u001b[39mprompt()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_story\u001b[39m(theme: \u001b[38;5;28mstr\u001b[39m, length: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    根据主题生成一个故事\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    主题：{{theme}}\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    长度：{{length}}\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m story \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_story\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheme\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m科幻\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(story)\n",
      "File \u001b[0;32m~/projects/byzer-llm/src/byzerllm/__init__.py:698\u001b[0m, in \u001b[0;36m_DescriptorPrompt.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/byzer-llm/src/byzerllm/__init__.py:602\u001b[0m, in \u001b[0;36m_PrompRunner.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm should be a lambda function or ByzerLLM instance or a string of model name\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: llm should be a lambda function or ByzerLLM instance or a string of model name"
     ]
    }
   ],
   "source": [
    "import byzerllm\n",
    "@byzerllm.prompt()\n",
    "def generate_story(theme: str, length: int) -> str:\n",
    "    \"\"\"\n",
    "    根据主题生成一个故事\n",
    "    主题：{{theme}}\n",
    "    长度：{{length}}\n",
    "    \"\"\"\n",
    "\n",
    "story = generate_story.with_llm(llm).run(theme=\"科幻\", length=200)\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理模型\n",
    "llm.undeploy(\"test_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
