{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test OpenAI SDK with Byzer-LLM Chat API\n",
    "\n",
    "This notebook demonstrates how to use the OpenAI SDK to call Byzer-LLM's chat API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Configure OpenAI client to use our local server\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"dummy\",  # The API key can be any string when using local server\n",
    "    base_url=\"http://localhost:8000/v1\"  # Point to local Byzer-LLM serve endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code", 
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream", 
     "text": [
      "Assistant response: Hello! I am an AI assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Test chat completion API\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Model name doesn't matter when using local server\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, who are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Assistant response: {chat_completion.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3, 
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: Hello\n",
      "Chunk 2: ! I am\n",
      "Chunk 3:  an AI\n",
      "Chunk 4:  assistant\n",
      "Chunk 5: . How can I help\n",
      "Chunk 6:  you today?\n"
     ]
    }
   ],
   "source": [
    "# Test streaming chat completion API\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, who are you?\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Print each chunk as it arrives\n",
    "for i, chunk in enumerate(stream, 1):\n",
    "    chunk_content = chunk.choices[0].delta.content\n",
    "    if chunk_content:\n",
    "        print(f\"Chunk {i}: {chunk_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model list: ['gpt-3.5-turbo', 'gpt-4']\n"
     ]
    }
   ],
   "source": [
    "# Test models list API\n",
    "models = client.models.list()\n",
    "model_names = [model.id for model in models.data]\n",
    "print(f\"Model list: {model_names}\")"
   ]
  }
 ],
 "metadata": {\n",
  "kernelspec": {\n",
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n",
  },\n",
  "language_info": {\n",
   "codemirror_mode": {\n",
    "name": "ipython",\n",
    "version": 3\n",
   },\n",
   "file_extension": ".py",\n",
   "mimetype": "text/x-python",\n",
   "name": "python",\n",
   "nbconvert_exporter": "python",\n",
   "pygments_lexer": "ipython3",\n",
   "version": "3.10.11"\n",
  }\n",
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test OpenAI SDK with Byzer-LLM Chat API\n",
    "\n",
    "This notebook demonstrates how to use the OpenAI SDK to call Byzer-LLM's chat completion API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Configure OpenAI client to use our local server\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"dummy\",  # The API key can be any string when using local server\n",
    "    base_url=\"http://localhost:8000/v1\"  # Point to local Byzer-LLM serve endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code", 
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream", 
     "text": [
      "Assistant: Hello! I am an AI assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Test basic chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Model name doesn't matter when using local server\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Who are you?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3, 
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I'll do my best to explain this concept clearly:\n",
      "\n",
      "Machine learning (ML) is a branch of artificial intelligence that allows computers to learn and improve from experience without being explicitly programmed. Think of it like teaching a computer to recognize patterns in data, similar to how humans learn from examples.\n",
      "\n",
      "For instance, if you want to build an ML system to identify cats in photos:\n",
      "1. You show it thousands of cat photos (training data)\n",
      "2. The system learns the patterns that make up a cat (learning)\n",
      "3. It can then recognize cats in new photos it's never seen before (prediction)\n",
      "\n",
      "Would you like me to explain any specific aspect of machine learning in more detail?\n"
     ]
    }
   ],
   "source": [
    "# Test multi-turn conversation\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain what machine learning is?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'll do my best to explain this concept clearly...\"}, \n",
    "        {\"role\": \"user\", \"content\": \"Great explanation! Can you give a simple example?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Assistant: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response chunks:\n",
      "Hello! I am an AI \n",
      "assistant. I'm here to\n",
      " help answer your questions\n",
      " and assist you with\n",
      " various tasks.\n"
     ]
    }
   ],
   "source": [
    "# Test streaming chat completion\n",
    "print(\"Streaming response chunks:\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello! Who are you?\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {\n",
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n",
  },
  "language_info": {\n",
   "codemirror_mode": {\n",
    "name": "ipython",\n",
    "version": 3\n",
   },
   "file_extension": ".py",\n",
   "mimetype": "text/x-python",\n",
   "name": "python",\n",
   "nbconvert_exporter": "python",\n",
   "pygments_lexer": "ipython3",\n",
   "version": "3.10.11"\n",
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}